#!/usr/bin/env python3
"""
Script d'√©valuation pour le benchmark d'analyse rh√©torique.

Compare les annotations du mod√®le avec les annotations de r√©f√©rence (gold standard)
et calcule des m√©triques de performance.

Usage:
    python evaluate.py gold.json predicted.json
    python evaluate.py --batch benchmark/annotations/gold/ benchmark/annotations/model/
"""

import argparse
import csv
import json
from collections import defaultdict
from dataclasses import dataclass, field
from pathlib import Path


@dataclass
class EvaluationMetrics:
    """M√©triques d'√©valuation pour une analyse."""

    # D√©tection de sophismes
    fallacy_precision: float = 0.0
    fallacy_recall: float = 0.0
    fallacy_f1: float = 0.0
    fallacy_details: dict[str, dict[str, float]] = field(default_factory=dict)

    # Score de fiabilit√©
    reliability_mae: float = 0.0  # Mean Absolute Error
    reliability_correlation: float = 0.0

    # Nombre d'arguments
    argument_count_gold: int = 0
    argument_count_predicted: int = 0

    # Composants Toulmin (si annot√©s)
    toulmin_claim_match: float = 0.0
    toulmin_grounds_match: float = 0.0

    def to_dict(self) -> dict:
        return {
            "fallacy_precision": round(self.fallacy_precision, 3),
            "fallacy_recall": round(self.fallacy_recall, 3),
            "fallacy_f1": round(self.fallacy_f1, 3),
            "reliability_mae": round(self.reliability_mae, 3),
            "argument_count_gold": self.argument_count_gold,
            "argument_count_predicted": self.argument_count_predicted,
        }


def extract_fallacies(analysis: dict) -> dict[int, list[str]]:
    """Extrait les sophismes par argument."""
    result = {}
    for arg in analysis.get("arguments", []):
        arg_id = arg.get("id", 0)
        fallacies = arg.get("fallacies", [])
        # Normaliser les noms de sophismes (lowercase, sans espaces)
        normalized = [f.lower().strip().replace(" ", "_") for f in fallacies if f]
        result[arg_id] = normalized
    return result


def extract_reliability_scores(analysis: dict) -> dict[int, int]:
    """Extrait les scores de fiabilit√© par argument."""
    result = {}
    for arg in analysis.get("arguments", []):
        arg_id = arg.get("id", 0)
        reliability = arg.get("reliability", 3)
        result[arg_id] = reliability
    return result


def compute_fallacy_metrics(
    gold_fallacies: dict[int, list[str]], pred_fallacies: dict[int, list[str]]
) -> tuple[float, float, float, dict]:
    """
    Calcule precision, recall, F1 pour la d√©tection de sophismes.

    On consid√®re une d√©tection correcte si:
    - Le m√™me argument est identifi√©
    - Le m√™me type de sophisme est d√©tect√©
    """
    true_positives = 0
    false_positives = 0
    false_negatives = 0

    details_by_type = defaultdict(lambda: {"tp": 0, "fp": 0, "fn": 0})

    # Aligner par argument ID
    all_arg_ids = set(gold_fallacies.keys()) | set(pred_fallacies.keys())

    for arg_id in all_arg_ids:
        gold_set = set(gold_fallacies.get(arg_id, []))
        pred_set = set(pred_fallacies.get(arg_id, []))

        # True positives: dans gold ET dans pred
        tp = gold_set & pred_set
        true_positives += len(tp)
        for f in tp:
            details_by_type[f]["tp"] += 1

        # False positives: dans pred mais pas dans gold
        fp = pred_set - gold_set
        false_positives += len(fp)
        for f in fp:
            details_by_type[f]["fp"] += 1

        # False negatives: dans gold mais pas dans pred
        fn = gold_set - pred_set
        false_negatives += len(fn)
        for f in fn:
            details_by_type[f]["fn"] += 1

    # Calcul des m√©triques globales
    precision = (
        true_positives / (true_positives + false_positives)
        if (true_positives + false_positives) > 0
        else 0
    )
    recall = (
        true_positives / (true_positives + false_negatives)
        if (true_positives + false_negatives) > 0
        else 0
    )
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    # Calcul par type
    details = {}
    for fallacy_type, counts in details_by_type.items():
        tp, fp, fn = counts["tp"], counts["fp"], counts["fn"]
        p = tp / (tp + fp) if (tp + fp) > 0 else 0
        r = tp / (tp + fn) if (tp + fn) > 0 else 0
        f = 2 * p * r / (p + r) if (p + r) > 0 else 0
        details[fallacy_type] = {"precision": p, "recall": r, "f1": f}

    return precision, recall, f1, details


def compute_reliability_mae(gold_scores: dict[int, int], pred_scores: dict[int, int]) -> float:
    """Calcule le Mean Absolute Error sur les scores de fiabilit√©."""
    common_ids = set(gold_scores.keys()) & set(pred_scores.keys())

    if not common_ids:
        return 0.0

    total_error = sum(abs(gold_scores[i] - pred_scores[i]) for i in common_ids)
    return total_error / len(common_ids)


def evaluate_analysis(gold: dict, predicted: dict) -> EvaluationMetrics:
    """
    √âvalue une analyse pr√©dite par rapport √† la r√©f√©rence gold.

    Args:
        gold: Annotation de r√©f√©rence (format JSON du skill)
        predicted: Annotation du mod√®le

    Returns:
        EvaluationMetrics avec toutes les m√©triques calcul√©es
    """
    metrics = EvaluationMetrics()

    # Nombre d'arguments
    metrics.argument_count_gold = len(gold.get("arguments", []))
    metrics.argument_count_predicted = len(predicted.get("arguments", []))

    # Extraction des donn√©es
    gold_fallacies = extract_fallacies(gold)
    pred_fallacies = extract_fallacies(predicted)

    gold_reliability = extract_reliability_scores(gold)
    pred_reliability = extract_reliability_scores(predicted)

    # Calcul des m√©triques de sophismes
    precision, recall, f1, details = compute_fallacy_metrics(gold_fallacies, pred_fallacies)
    metrics.fallacy_precision = precision
    metrics.fallacy_recall = recall
    metrics.fallacy_f1 = f1
    metrics.fallacy_details = details

    # Calcul du MAE sur la fiabilit√©
    metrics.reliability_mae = compute_reliability_mae(gold_reliability, pred_reliability)

    return metrics


def print_report(metrics: EvaluationMetrics, verbose: bool = True) -> None:
    """Affiche un rapport format√© des m√©triques."""
    print("\n" + "=" * 60)
    print("RAPPORT D'√âVALUATION")
    print("=" * 60)

    print("\nüìä Arguments identifi√©s:")
    print(f"   Gold: {metrics.argument_count_gold}")
    print(f"   Mod√®le: {metrics.argument_count_predicted}")

    print("\nüéØ D√©tection de sophismes:")
    print(f"   Precision: {metrics.fallacy_precision:.2%}")
    print(f"   Recall: {metrics.fallacy_recall:.2%}")
    print(f"   F1: {metrics.fallacy_f1:.2%}")

    if verbose and metrics.fallacy_details:
        print("\n   D√©tail par type:")
        for fallacy_type, scores in sorted(metrics.fallacy_details.items()):
            print(
                f"   - {fallacy_type}: P={scores['precision']:.2f} R={scores['recall']:.2f} F1={scores['f1']:.2f}"
            )

    print("\nüìè Score de fiabilit√©:")
    print(f"   MAE: {metrics.reliability_mae:.2f} (sur √©chelle 1-5)")

    # Interpr√©tation
    print("\n" + "-" * 60)
    print("INTERPR√âTATION:")
    if metrics.fallacy_f1 >= 0.7:
        print("‚úÖ Bonne d√©tection des sophismes")
    elif metrics.fallacy_f1 >= 0.5:
        print("‚ö†Ô∏è  D√©tection des sophismes √† am√©liorer")
    else:
        print("‚ùå D√©tection des sophismes insuffisante")

    if metrics.reliability_mae <= 0.5:
        print("‚úÖ Scores de fiabilit√© bien calibr√©s")
    elif metrics.reliability_mae <= 1.0:
        print("‚ö†Ô∏è  Scores de fiabilit√© √† affiner")
    else:
        print("‚ùå Scores de fiabilit√© √† recalibrer")

    print("=" * 60 + "\n")


def batch_evaluate(gold_dir: Path, pred_dir: Path) -> list[tuple[str, EvaluationMetrics]]:
    """√âvalue tous les fichiers d'un r√©pertoire."""
    results = []

    for gold_file in gold_dir.glob("*.json"):
        pred_file = pred_dir / gold_file.name
        if pred_file.exists():
            with open(gold_file) as f:
                gold = json.load(f)
            with open(pred_file) as f:
                pred = json.load(f)

            metrics = evaluate_analysis(gold, pred)
            results.append((gold_file.stem, metrics))
        else:
            print(f"‚ö†Ô∏è  Pas de pr√©diction pour {gold_file.name}")

    return results


def export_results_csv(results: list[tuple[str, EvaluationMetrics]], output_path: Path) -> None:
    """Exporte les r√©sultats en CSV."""
    with open(output_path, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(
            [
                "article",
                "fallacy_precision",
                "fallacy_recall",
                "fallacy_f1",
                "reliability_mae",
                "arg_count_gold",
                "arg_count_pred",
            ]
        )
        for name, metrics in results:
            writer.writerow(
                [
                    name,
                    f"{metrics.fallacy_precision:.3f}",
                    f"{metrics.fallacy_recall:.3f}",
                    f"{metrics.fallacy_f1:.3f}",
                    f"{metrics.reliability_mae:.3f}",
                    metrics.argument_count_gold,
                    metrics.argument_count_predicted,
                ]
            )
    print(f"‚úÖ R√©sultats export√©s dans {output_path}")


def main():
    parser = argparse.ArgumentParser(description="√âvaluation d'analyses rh√©toriques")
    parser.add_argument("gold", help="Fichier/r√©pertoire gold standard")
    parser.add_argument("predicted", help="Fichier/r√©pertoire des pr√©dictions")
    parser.add_argument("--batch", action="store_true", help="Mode batch (r√©pertoires)")
    parser.add_argument("--output", "-o", help="Fichier CSV de sortie (mode batch)")
    parser.add_argument("--quiet", "-q", action="store_true", help="Mode silencieux")

    args = parser.parse_args()

    if args.batch:
        gold_dir = Path(args.gold)
        pred_dir = Path(args.predicted)

        results = batch_evaluate(gold_dir, pred_dir)

        if not args.quiet:
            print(f"\nüìÅ √âvaluation de {len(results)} fichiers\n")
            for name, metrics in results:
                print(f"--- {name} ---")
                print_report(metrics, verbose=False)

        if args.output:
            export_results_csv(results, Path(args.output))

        # Moyennes globales
        if results:
            avg_f1 = sum(m.fallacy_f1 for _, m in results) / len(results)
            avg_mae = sum(m.reliability_mae for _, m in results) / len(results)
            print("\nüìä MOYENNES GLOBALES:")
            print(f"   Fallacy F1: {avg_f1:.2%}")
            print(f"   Reliability MAE: {avg_mae:.2f}")
    else:
        with open(args.gold) as f:
            gold = json.load(f)
        with open(args.predicted) as f:
            predicted = json.load(f)

        metrics = evaluate_analysis(gold, predicted)

        if not args.quiet:
            print_report(metrics)

        # Retour JSON pour int√©gration
        print(json.dumps(metrics.to_dict(), indent=2))


if __name__ == "__main__":
    main()
